{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import the required libraries\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "from dotenv import dotenv_values\n",
    "from psycopg2 import DatabaseError, InterfaceError, OperationalError\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Load the configuration from .env into context\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# DB Variables\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']\n",
    "pg_user = config['POSTGRES_USER']\n",
    "pg_password = config['POSTGRES_PASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Source Files Path    \n",
    "files_path_digi_sa = config['DATA_FILES_PATH_DIGI_SA']\n",
    "\n",
    "# DB Connection String\n",
    "DATABASE_URL = f\"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Data Cleaning Variables\n",
    "check_duplicates_data = ['country_name', 'country_code', 'indicator_code']\n",
    "check_duplicates_countries = ['country_code', 'region']\n",
    "check_duplicates_indicators = ['indicator_code']\n",
    "\n",
    "unused_columns = ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
    "\n",
    "column_data_types = {\n",
    "    'country_name': 'str', 'country_code': 'str', 'indicator_name': 'str', 'indicator_code': 'str', 'year': 'int', 'indicator_value': 'float',\n",
    "    'region': 'str', 'income_group': 'str', 'special_notes': 'str', 'table_name': 'str',\n",
    "    'source_note': 'str', 'source_organization': 'str'\n",
    "}\n",
    "\n",
    "sheet_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Class to encapsulate\n",
    "class DataFrameOperations:\n",
    "    def convert_columns_lower(self, sheet_df):\n",
    "        logging.info(\"Converting Columns to Lowercase\")\n",
    "        sheet_df.columns = sheet_df.columns.str.replace(' ', '_').str.replace(r'(?<!^)(?=[A-Z])', '_').str.lower()\n",
    "        return sheet_df    \n",
    "\n",
    "    def remove_duplicates(self, sheet_df, columns):\n",
    "        logging.info(f\"Duplicates before removal: {sheet_df.duplicated(subset=columns, keep='first').sum()}\")\t\n",
    "        sheet_df.drop_duplicates(subset=columns, keep='first', inplace=True)\n",
    "        return sheet_df\n",
    "    \n",
    "    def remove_unused_columns(self, sheet_df, columns):\n",
    "        logging.info(f\"Removing Columns: {columns}\")\n",
    "        sheet_df.drop(columns, axis=1, inplace=True, errors='ignore')\n",
    "        return sheet_df\n",
    "    \n",
    "    def melt_data(self, sheet_df, id_vars, var_name, value_name):\n",
    "        logging.info(\"Melting Data\")\n",
    "        sheet_df = pd.melt(sheet_df, id_vars=id_vars, var_name=var_name, value_name=value_name)\n",
    "        logging.info(f\"Columns after Melting Data: {sheet_df.columns}\")\n",
    "        return sheet_df    \n",
    "\n",
    "    def replace_colvalues(self, sheet_df, columns):\n",
    "        logging.info(f\"Before Replacing Column Values: {sheet_df[columns].head(1)}\")\n",
    "        sheet_df[columns] = sheet_df[columns].str.replace(r\"(\\d{4})_\\[yr\\d{4}\\]\", r\"\\1\", regex=True)\n",
    "        logging.info(f\"After Replacing Column Values: {sheet_df[columns].head(1)}\")\n",
    "        return sheet_df\n",
    "\n",
    "    def interpolate_na_rows(self, sheet_df, columns):\n",
    "        logging.info(f\"Interpolating N/A Columns: {columns}\")\n",
    "        sheet_df[columns].interpolate(method='polynomial', order=2, inplace=True)\n",
    "        return sheet_df\n",
    "    \n",
    "    def standardize_data_types(self, sheet_df, columns):\n",
    "        logging.info(\"Standardizing Data Types\")\n",
    "        for column, dtype in columns.items():\n",
    "            if column in sheet_df.columns:\n",
    "                sheet_df[column] = sheet_df[column].astype(dtype)\n",
    "        return sheet_df  \n",
    "\n",
    "    def convert_datetime(self, sheet_df, columns):\n",
    "        logging.info(f\"Converting Datetime Columns: {columns}\")\n",
    "        sheet_df[columns] = pd.to_datetime(sheet_df[columns], dayfirst=True)\n",
    "        return sheet_df\n",
    "    \n",
    "    def calculate_lead_times(self, sheet_df, col_A, col_B):\n",
    "        return (sheet_df[col_B] - sheet_df[col_A]).dt.days\n",
    "\n",
    "    def perform_db_ops(self, DATABASE_URL, sheet_dict):\n",
    "        logging.info(f\"Writing data to DB {sheet_dict.keys()}\")\n",
    "        engine = create_engine(DATABASE_URL) #, connect_args={\"connect_timeout\": 100})\n",
    "        try:\n",
    "            for table_name, database_df in sheet_dict.items():\n",
    "                table_name = f\"{table_name}_digi_sa\"\n",
    "                logging.info(f\"Table Name: {table_name}\")\n",
    "                start_time = time.time()\n",
    "                logging.info(f\"Start Time: {start_time}\")\n",
    "                logging.info(f\"Engine: {engine}\")\n",
    "                logging.info(f\"Schema: {pg_schema}\")\n",
    "                database_df.to_sql(table_name, engine, schema=pg_schema, if_exists='replace', index=False) #, chunksize=1000, method='multi')\n",
    "                end_time = time.time()\n",
    "                logging.info(f\"DB write duration: {end_time - start_time:.2f} seconds\")\n",
    "                logging.info(\"Data successfully imported to the PostgreSQL database.\")\n",
    "                with engine.connect() as connection:\n",
    "                    result = connection.execute(text(f\"SELECT count(*) FROM {pg_schema}.{table_name};\"))\n",
    "                    logging.info(f\"Count of Rows in table {table_name}: {result.scalar()}\")\n",
    "        except (OperationalError, InterfaceError, DatabaseError) as db_err:\n",
    "            logging.error(f\"Database error occurred: {db_err}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        finally: \n",
    "            engine.dispose()\n",
    "            logging.info(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 15:41:06,648 - INFO - Processing file: P_Data_Extract_From_Africa_Development_Indicators.xls\n",
      "2024-10-29 15:41:10,981 - INFO - Sheet Names: dict_keys(['Data', 'Metadata - Countries', 'Metadata - Indicators', 'Previous_Data'])\n",
      "2024-10-29 15:41:10,982 - INFO - Processing Sheet: Data\n",
      "2024-10-29 15:41:10,983 - INFO - Converting Columns to Lowercase\n",
      "2024-10-29 15:41:10,990 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-29 15:41:11,023 - INFO - Duplicates before removal: 0\n",
      "2024-10-29 15:41:11,032 - INFO - Melting Data\n",
      "2024-10-29 15:41:11,140 - INFO - Columns after Melting Data: Index(['country_name', 'country_code', 'indicator_name', 'indicator_code',\n",
      "       'year', 'indicator_value'],\n",
      "      dtype='object')\n",
      "2024-10-29 15:41:11,152 - INFO - Before Replacing Column Values: 0    1990_[yr1990]\n",
      "Name: year, dtype: object\n",
      "2024-10-29 15:41:11,192 - INFO - After Replacing Column Values: 0    1990\n",
      "Name: year, dtype: object\n",
      "2024-10-29 15:41:11,200 - INFO - Standardizing Data Types\n",
      "2024-10-29 15:41:11,216 - INFO - Interpolating N/A Columns: indicator_value\n",
      "/var/folders/r_/l5qrwcxs1mj5p7h8037l8hk80000gn/T/ipykernel_61761/3279703172.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sheet_df[columns].interpolate(method='polynomial', order=2, inplace=True)\n",
      "2024-10-29 15:41:11,846 - INFO - Processing Sheet: Metadata - Countries\n",
      "2024-10-29 15:41:11,847 - INFO - Converting Columns to Lowercase\n",
      "2024-10-29 15:41:11,848 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-29 15:41:11,851 - INFO - Duplicates before removal: 0\n",
      "2024-10-29 15:41:11,855 - INFO - Standardizing Data Types\n",
      "2024-10-29 15:41:11,857 - INFO - Processing Sheet: Metadata - Indicators\n",
      "2024-10-29 15:41:11,858 - INFO - Converting Columns to Lowercase\n",
      "2024-10-29 15:41:11,868 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-29 15:41:11,875 - INFO - Duplicates before removal: 9\n",
      "2024-10-29 15:41:11,902 - INFO - Standardizing Data Types\n",
      "2024-10-29 15:41:11,907 - INFO - Processing Sheet: Previous_Data\n",
      "2024-10-29 15:41:11,908 - INFO - Converting Columns to Lowercase\n",
      "2024-10-29 15:41:11,925 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-29 15:41:11,974 - INFO - Duplicates before removal: 0\n",
      "2024-10-29 15:41:12,023 - INFO - Melting Data\n",
      "2024-10-29 15:41:12,305 - INFO - Columns after Melting Data: Index(['country_name', 'country_code', 'indicator_name', 'indicator_code',\n",
      "       'year', 'indicator_value'],\n",
      "      dtype='object')\n",
      "2024-10-29 15:41:12,306 - INFO - Before Replacing Column Values: 0    1960\n",
      "Name: year, dtype: object\n",
      "2024-10-29 15:41:13,335 - INFO - After Replacing Column Values: 0    1960\n",
      "Name: year, dtype: object\n",
      "2024-10-29 15:41:13,335 - INFO - Standardizing Data Types\n",
      "2024-10-29 15:41:13,638 - INFO - Interpolating N/A Columns: indicator_value\n",
      "/var/folders/r_/l5qrwcxs1mj5p7h8037l8hk80000gn/T/ipykernel_61761/3279703172.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sheet_df[columns].interpolate(method='polynomial', order=2, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the class\n",
    "df_ops = DataFrameOperations()\n",
    "\n",
    "# Loop over each sheet\n",
    "for filename in os.listdir(files_path_digi_sa):\n",
    "    logging.info(f\"Processing file: {filename}\")\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(files_path_digi_sa, filename)\n",
    "    \n",
    "    try:\n",
    "        sheets_dict = pd.read_excel(file_path, sheet_name=None, engine=\"xlrd\")\n",
    "        logging.info(f\"Sheet Names: {sheets_dict.keys()}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading the file: {e}\")\n",
    "        continue\n",
    "    \n",
    "    for sheet_name, sheet_df in sheets_dict.items():\n",
    "        logging.info(f\"Processing Sheet: {sheet_name}\")\n",
    "        \n",
    "        # Common cleansing for all sheets\n",
    "        sheet_df = df_ops.convert_columns_lower(sheet_df)\n",
    "        sheet_df = df_ops.remove_unused_columns(sheet_df, unused_columns) \n",
    "        # Sheet specific cleansing\n",
    "        if sheet_name in ['Data', 'Previous_Data']:\n",
    "            sheet_df = df_ops.remove_duplicates(sheet_df, check_duplicates_data)            \n",
    "            sheet_df = df_ops.melt_data(sheet_df, ['country_name', 'country_code', 'indicator_name', 'indicator_code'], 'year', 'indicator_value')\n",
    "            sheet_df = df_ops.replace_colvalues(sheet_df, 'year')\n",
    "            sheet_df = df_ops.standardize_data_types(sheet_df, column_data_types)\n",
    "            sheet_df = df_ops.interpolate_na_rows(sheet_df, 'indicator_value')          \n",
    "            sheet_dict[sheet_name.lower()] = sheet_df             \n",
    "            \n",
    "        elif sheet_name == 'Metadata - Countries':\n",
    "            sheet_df = df_ops.remove_duplicates(sheet_df, check_duplicates_countries)\n",
    "            sheet_df = df_ops.standardize_data_types(sheet_df, column_data_types)\n",
    "            sheet_dict[sheet_name.split('-')[1].lower().strip()] = sheet_df           \n",
    "\n",
    "        elif sheet_name == 'Metadata - Indicators':\n",
    "            sheet_df = df_ops.remove_duplicates(sheet_df, check_duplicates_indicators)\n",
    "            sheet_df = df_ops.standardize_data_types(sheet_df, column_data_types)\n",
    "            sheet_dict[sheet_name.split('-')[1].lower().strip()] = sheet_df\n",
    "      \n",
    "        else:\n",
    "            logging.warning(f\"Sheet {sheet_name} not found\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 15:41:14,092 - INFO - Writing data to DB dict_keys(['data', 'countries', 'indicators', 'previous_data'])\n",
      "2024-10-29 15:41:14,460 - INFO - Table Name: data_digi_sa\n",
      "2024-10-29 15:41:14,466 - INFO - Start Time: 1730212874.466476\n",
      "2024-10-29 15:41:14,468 - INFO - Engine: Engine(postgresql://shaunkutsanzira:***@data-analytics-course-2.c8g8r1deus2v.eu-central-1.rds.amazonaws.com:5432/hh_analytics_24_2)\n",
      "2024-10-29 15:41:14,475 - INFO - Schema: capstone_digital_transformation_impact\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_ops.perform_db_ops(DATABASE_URL, sheet_dict)\n",
    "logging.info(f\"Data Processing Completed for file: {filename}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(files_path_digi_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
