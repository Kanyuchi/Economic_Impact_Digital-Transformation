{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 09:03:52,668 - INFO - Processing file: P_Data_Extract_From_Africa_Development_Indicators.xls\n",
      "2024-10-27 09:03:54,429 - INFO - Sheet Names: dict_keys(['Data', 'Metadata - Countries', 'Metadata - Indicators', 'Previous_Data'])\n",
      "2024-10-27 09:03:54,441 - INFO - Processing Sheet: Data\n",
      "2024-10-27 09:03:54,442 - INFO - Converting Columns to Lowercase\n",
      "2024-10-27 09:03:54,443 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-27 09:03:54,445 - INFO - Processing sheet: Data\n",
      "2024-10-27 09:03:54,447 - INFO - Duplicates before removal: 0\n",
      "2024-10-27 09:03:54,448 - INFO - Melting Data\n",
      "2024-10-27 09:03:54,456 - INFO - Columns after Melting Data: Index(['country_name', 'country_code', 'indicator_name', 'indicator_code', 'year', 'indicator_value'], dtype='object')\n",
      "2024-10-27 09:03:54,457 - INFO - Before Replacing Column Values: 0    1990_[yr1990]\n",
      "Name: year, dtype: object\n",
      "2024-10-27 09:03:54,511 - INFO - After Replacing Column Values: 0    1990\n",
      "Name: year, dtype: object\n",
      "2024-10-27 09:03:54,511 - INFO - Standardizing Data Types\n",
      "2024-10-27 09:03:54,517 - INFO - Interpolating N/A Columns: indicator_value\n",
      "2024-10-27 09:03:54,522 - INFO - Processing Sheet: Metadata - Countries\n",
      "2024-10-27 09:03:54,522 - INFO - Converting Columns to Lowercase\n",
      "2024-10-27 09:03:54,523 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-27 09:03:54,524 - INFO - Processing sheet: Metadata - Countries\n",
      "2024-10-27 09:03:54,526 - INFO - Duplicates before removal: 0\n",
      "2024-10-27 09:03:54,528 - INFO - Standardizing Data Types\n",
      "2024-10-27 09:03:54,531 - INFO - Processing Sheet: Metadata - Indicators\n",
      "2024-10-27 09:03:54,531 - INFO - Converting Columns to Lowercase\n",
      "2024-10-27 09:03:54,532 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-27 09:03:54,536 - INFO - Processing sheet: Metadata - Indicators\n",
      "2024-10-27 09:03:54,538 - INFO - Duplicates before removal: 9\n",
      "2024-10-27 09:03:54,540 - INFO - Standardizing Data Types\n",
      "2024-10-27 09:03:54,542 - INFO - Processing Sheet: Previous_Data\n",
      "2024-10-27 09:03:54,543 - INFO - Converting Columns to Lowercase\n",
      "2024-10-27 09:03:54,544 - INFO - Removing Columns: ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
      "2024-10-27 09:03:54,549 - INFO - Processing sheet: Previous_Data\n",
      "2024-10-27 09:03:54,565 - INFO - Duplicates before removal: 0\n",
      "2024-10-27 09:03:54,572 - INFO - Melting Data\n",
      "2024-10-27 09:03:54,649 - INFO - Columns after Melting Data: Index(['country_name', 'country_code', 'indicator_name', 'indicator_code', 'year', 'indicator_value'], dtype='object')\n",
      "2024-10-27 09:03:54,651 - INFO - Before Replacing Column Values: 0    1960\n",
      "Name: year, dtype: object\n",
      "2024-10-27 09:03:56,539 - INFO - After Replacing Column Values: 0    1960\n",
      "Name: year, dtype: object\n",
      "2024-10-27 09:03:56,540 - INFO - Standardizing Data Types\n",
      "2024-10-27 09:03:56,738 - INFO - Interpolating N/A Columns: indicator_value\n",
      "2024-10-27 09:03:56,931 - INFO - Length of Sheet Data List: 4\n",
      "2024-10-27 09:03:56,932 - INFO - Writing data to DB dict_keys(['data', 'countries', 'indicators', 'previous_data'])\n",
      "2024-10-27 09:03:56,933 - INFO - Table Name: data_digi_sa\n",
      "2024-10-27 09:04:01,334 - INFO - DB write duration: 4.40 seconds\n",
      "2024-10-27 09:04:01,335 - INFO - Data successfully imported to the PostgreSQL database.\n",
      "2024-10-27 09:04:01,359 - INFO - Count of Rows in table data_digi_sa: 17940\n",
      "2024-10-27 09:04:01,372 - INFO - Table Name: countries_digi_sa\n",
      "2024-10-27 09:04:01,656 - INFO - DB write duration: 0.28 seconds\n",
      "2024-10-27 09:04:01,657 - INFO - Data successfully imported to the PostgreSQL database.\n",
      "2024-10-27 09:04:01,686 - INFO - Count of Rows in table countries_digi_sa: 13\n",
      "2024-10-27 09:04:01,698 - INFO - Table Name: indicators_digi_sa\n",
      "2024-10-27 09:04:02,235 - INFO - DB write duration: 0.54 seconds\n",
      "2024-10-27 09:04:02,236 - INFO - Data successfully imported to the PostgreSQL database.\n",
      "2024-10-27 09:04:02,260 - INFO - Count of Rows in table indicators_digi_sa: 1500\n",
      "2024-10-27 09:04:02,272 - INFO - Table Name: previous_data_digi_sa\n",
      "2024-10-27 09:08:10,900 - INFO - DB write duration: 248.63 seconds\n",
      "2024-10-27 09:08:10,901 - INFO - Data successfully imported to the PostgreSQL database.\n",
      "2024-10-27 09:08:11,368 - INFO - Count of Rows in table previous_data_digi_sa: 1244672\n",
      "2024-10-27 09:08:11,382 - INFO - Database connection closed.\n",
      "2024-10-27 09:08:11,384 - INFO - Data Processing Completed for file: P_Data_Extract_From_Africa_Development_Indicators.xls\n"
     ]
    }
   ],
   "source": [
    "# Let us import the required libraries\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "from dotenv import dotenv_values\n",
    "from psycopg2 import DatabaseError, InterfaceError, OperationalError\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Load the configuration from .env into context\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# DB Variables\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']\n",
    "pg_user = config['POSTGRES_USER']\n",
    "pg_password = config['POSTGRES_PASS']\n",
    "\n",
    "# Data Source Files Path    \n",
    "files_path_digi_sa = config['DATA_FILES_PATH_DIGI_SA']\n",
    "\n",
    "# DB Connection String\n",
    "DATABASE_URL = f\"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Data Cleaning Variables\n",
    "check_duplicates_data = ['country_name', 'country_code', 'indicator_code']\n",
    "check_duplicates_countries = ['country_code', 'region']\n",
    "check_duplicates_indicators = ['indicator_code']\n",
    "\n",
    "unused_columns = ['scale_precision', 'table_name', 'long_defintion', 'topic', 'periodicity', 'aggregation_method', 'general_comments']\n",
    "\n",
    "column_data_types = {\n",
    "    'country_name': 'str', 'country_code': 'str', 'indicator_name': 'str', 'indicator_code': 'str', 'year': 'int', 'indicator_value': 'float',\n",
    "    'region': 'str', 'income_group': 'str', 'special_notes': 'str', 'table_name': 'str',\n",
    "    'source_note': 'str', 'source_organization': 'str'\n",
    "}\n",
    "\n",
    "sheet_dict = {}\n",
    "\n",
    "# Creating Class to encapsulate\n",
    "class DataFrameOperations:\n",
    "    def convert_columns_lower(self, sheet_df):\n",
    "        logging.info(\"Converting Columns to Lowercase\")\n",
    "        sheet_df.columns = sheet_df.columns.str.replace(' ', '_').str.replace(r'(?<!^)(?=[A-Z])', '_').str.lower()\n",
    "        return sheet_df    \n",
    "\n",
    "    def remove_duplicates(self, sheet_df, columns):\n",
    "        logging.info(f\"Duplicates before removal: {sheet_df.duplicated(subset=columns, keep='first').sum()}\")\t\n",
    "        sheet_df.drop_duplicates(subset=columns, keep='first', inplace=True)\n",
    "        return sheet_df\n",
    "    \n",
    "    def remove_unused_columns(self, sheet_df, columns):\n",
    "        logging.info(f\"Removing Columns: {columns}\")\n",
    "        sheet_df.drop(columns, axis=1, inplace=True, errors='ignore')\n",
    "        return sheet_df\n",
    "    \n",
    "    def melt_data(self, sheet_df, id_vars, var_name, value_name):\n",
    "        logging.info(\"Melting Data\")\n",
    "        sheet_df = pd.melt(sheet_df, id_vars=id_vars, var_name=var_name, value_name=value_name)\n",
    "        logging.info(f\"Columns after Melting Data: {sheet_df.columns}\")\n",
    "        return sheet_df    \n",
    "\n",
    "    def replace_colvalues(self, sheet_df, columns):\n",
    "        logging.info(f\"Before Replacing Column Values: {sheet_df[columns].head(1)}\")\n",
    "        sheet_df[columns] = sheet_df[columns].str.replace(r\"(\\d{4})_\\[yr\\d{4}\\]\", r\"\\1\", regex=True)\n",
    "        logging.info(f\"After Replacing Column Values: {sheet_df[columns].head(1)}\")\n",
    "        return sheet_df\n",
    "\n",
    "    def interpolate_na_rows(self, sheet_df, columns):\n",
    "        logging.info(f\"Interpolating N/A Columns: {columns}\")\n",
    "        sheet_df[columns].interpolate(method='polynomial', order=2, inplace=True)\n",
    "        return sheet_df\n",
    "    \n",
    "    def standardize_data_types(self, sheet_df, columns):\n",
    "        logging.info(\"Standardizing Data Types\")\n",
    "        for column, dtype in columns.items():\n",
    "            if column in sheet_df.columns:\n",
    "                sheet_df[column] = sheet_df[column].astype(dtype)\n",
    "        return sheet_df  \n",
    "\n",
    "    def convert_datetime(self, sheet_df, columns):\n",
    "        logging.info(f\"Converting Datetime Columns: {columns}\")\n",
    "        sheet_df[columns] = pd.to_datetime(sheet_df[columns], dayfirst=True)\n",
    "        return sheet_df\n",
    "    \n",
    "    def calculate_lead_times(self, sheet_df, col_A, col_B):\n",
    "        return (sheet_df[col_B] - sheet_df[col_A]).dt.days\n",
    "\n",
    "    def perform_db_ops(self, DATABASE_URL, sheet_dict):\n",
    "        logging.info(f\"Writing data to DB {sheet_dict.keys()}\")\n",
    "        engine = create_engine(DATABASE_URL, connect_args={\"connect_timeout\": 5})\n",
    "        try:\n",
    "            for table_name, database_df in sheet_dict.items():\n",
    "                table_name = f\"{table_name}_digi_sa\"\n",
    "                logging.info(f\"Table Name: {table_name}\")\n",
    "                start_time = time.time()\n",
    "                database_df.to_sql(table_name, engine, schema=pg_schema, if_exists='replace', index=False, chunksize=1000, method='multi')\n",
    "                end_time = time.time()\n",
    "                logging.info(f\"DB write duration: {end_time - start_time:.2f} seconds\")\n",
    "                logging.info(\"Data successfully imported to the PostgreSQL database.\")\n",
    "                with engine.connect() as connection:\n",
    "                    result = connection.execute(text(f\"SELECT count(*) FROM {pg_schema}.{table_name};\"))\n",
    "                    logging.info(f\"Count of Rows in table {table_name}: {result.scalar()}\")\n",
    "        except (OperationalError, InterfaceError, DatabaseError) as db_err:\n",
    "            logging.error(f\"Database error occurred: {db_err}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        finally:\n",
    "            engine.dispose()\n",
    "            logging.info(\"Database connection closed.\")\n",
    "\n",
    "# Creating an instance of the class\n",
    "df_ops = DataFrameOperations()\n",
    "\n",
    "# Loop over each sheet\n",
    "for filename in os.listdir(files_path_digi_sa):\n",
    "    logging.info(f\"Processing file: {filename}\")\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(files_path_digi_sa, filename)\n",
    "    \n",
    "    try:\n",
    "        sheets_dict = pd.read_excel(file_path, sheet_name=None, engine=\"xlrd\")\n",
    "        logging.info(f\"Sheet Names: {sheets_dict.keys()}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading the file: {e}\")\n",
    "        continue\n",
    "    \n",
    "    for sheet_name, sheet_df in sheets_dict.items():\n",
    "        logging.info(f\"Processing Sheet: {sheet_name}\")\n",
    "        \n",
    "        # Common cleansing for all sheets\n",
    "        sheet_df = df_ops.convert_columns_lower(sheet_df)\n",
    "        sheet_df = df_ops.remove_unused_columns(sheet_df, unused_columns)      \n",
    "\n",
    "        # Sheet specific cleansing\n",
    "        if sheet_name in ['Data', 'Previous_Data']:\n",
    "            sheet_df = df_ops.remove_duplicates(sheet_df, check_duplicates_data)            \n",
    "            sheet_df = df_ops.melt_data(sheet_df, ['country_name', 'country_code', 'indicator_name', 'indicator_code'], 'year', 'indicator_value')\n",
    "            sheet_df = df_ops.replace_colvalues(sheet_df, 'year')\n",
    "            sheet_df = df_ops.standardize_data_types(sheet_df, column_data_types)\n",
    "            sheet_df = df_ops.interpolate_na_rows(sheet_df, 'indicator_value')          \n",
    "            sheet_dict[sheet_name.lower()] = sheet_df             \n",
    "            \n",
    "        elif sheet_name == 'Metadata - Countries':\n",
    "            sheet_df = df_ops.remove_duplicates(sheet_df, check_duplicates_countries)\n",
    "            sheet_df = df_ops.standardize_data_types(sheet_df, column_data_types)\n",
    "            sheet_dict[sheet_name.split('-')[1].lower().strip()] = sheet_df           \n",
    "\n",
    "        elif sheet_name == 'Metadata - Indicators':\n",
    "            sheet_df = df_ops.remove_duplicates(sheet_df, check_duplicates_indicators)\n",
    "            sheet_df = df_ops.standardize_data_types(sheet_df, column_data_types)\n",
    "            sheet_dict[sheet_name.split('-')[1].lower().strip()] = sheet_df\n",
    "      \n",
    "        else:\n",
    "            logging.warning(f\"Sheet {sheet_name} not found\")\n",
    "\n",
    "    df_ops.perform_db_ops(DATABASE_URL, sheet_dict)\n",
    "    logging.info(f\"Data Processing Completed for file: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
